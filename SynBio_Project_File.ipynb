{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This is the test for initial commit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(\"Pandas imported successfully, version: \"+pd.__version__)\n",
    "import statsmodels as sm\n",
    "print(\"Statsmodels imported successfully, version: \"+sm.__version__)\n",
    "import plotly.graph_objects as go\n",
    "print(\"Plotly function imported succesfully\")\n",
    "import plotly.express as px\n",
    "print(\"Plotly express imported succesfully\")\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import plotly as py\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "print(\"Successful\")\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_csv('/Users/mazutislab/Desktop/SynBio/US_Accidents_May19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "data_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only needed data in the given dataset\n",
    "data_cleaned = data_all[[\"Severity\", \"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\", \n",
    "                        \"Precipitation(in)\"]]\n",
    "data_cleaned.reset_index()\n",
    "type(data_cleaned)\n",
    "\n",
    "#Convert Temperature in Fahrenheit to Celsius\n",
    "def fahr_to_celsius(temp_fahr):\n",
    "    \"\"\"Convert Fahrenheit to Celsius and Return Celsius conversion of input\"\"\"\n",
    "    temp_celsius = (temp_fahr - 32) * 5 / 9\n",
    "    return temp_celsius\n",
    "\n",
    "data_cleaned[\"Temperature(C)\"] = (fahr_to_celsius(data_cleaned[\"Temperature(F)\"])).round(2)\n",
    "data_cleaned.drop(['Temperature(F)'], inplace = True, axis = 1)\n",
    "\n",
    "#Convert Pressure in inches of mercury to mbar\n",
    "data_cleaned[\"Pressure(mbar)\"] = data_cleaned[\"Pressure(in)\"]*0.033863886666667*1000\n",
    "data_cleaned.drop(['Pressure(in)'], inplace = True, axis = 1)\n",
    "\n",
    "#Convert Wind Speed in mph to kmh\n",
    "data_cleaned[\"Wind_Speed(kmh)\"] = data_cleaned[\"Wind_Speed(mph)\"]*1.609344\n",
    "data_cleaned.drop(['Wind_Speed(mph)'], inplace = True, axis = 1)\n",
    "\n",
    "#Convert Precipitation in inches to mm\n",
    "data_cleaned[\"Precipitation(mm)\"] = data_cleaned[\"Precipitation(in)\"]*25.4\n",
    "data_cleaned.drop(['Precipitation(in)'], inplace = True, axis = 1)\n",
    "\n",
    "#Convert Visibility in miles to km\n",
    "data_cleaned[\"Visibility(km)\"] = data_cleaned[\"Visibility(mi)\"]*1.609344\n",
    "data_cleaned.drop(['Visibility(mi)'], inplace = True, axis = 1)\n",
    "data_cleaned.head()\n",
    "\n",
    "\n",
    "\n",
    "data_cleaned = data_cleaned[data_cleaned['Severity'] >= 1]\n",
    "data_cleaned = data_cleaned.sort_values(by=['Severity'], ascending = True)\n",
    "data_cleaned.reset_index()\n",
    "data_cleaned.head()\n",
    "\n",
    "t_col = 'Temperature(C)'\n",
    "h_col = 'Humidity(%)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Histogram(x=(data_cleaned['Severity']))])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#Frequency of severities\n",
    "Severity_frequency = {}\n",
    "\n",
    "for i, u in enumerate(range(0,5)):\n",
    "    Severity_frequency['Severity' + str(u)] = (data_cleaned[(data_cleaned['Severity']==i)]).shape[0]\n",
    "\n",
    "print(Severity_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset slicing function - Universal - for outliers removal\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + str(list(data_cleaned.columns)) + \" : \")\n",
    "\n",
    "lower_value = int(input(\"Enter lowest included value in your chosen column for your new dataset \"))\n",
    "higher_value = int(input(\"Enter highest included value in your chosen column for your new dataset \"))\n",
    "data_cleaned_bool = data_cleaned[column_name].between(lower_value, higher_value, inclusive = True)\n",
    "data_cleaned_new = data_cleaned[data_cleaned_bool]\n",
    "print(str(((data_cleaned[data_cleaned[column_name] > higher_value]).shape[0])+\n",
    "          ((data_cleaned[data_cleaned[column_name] < lower_value]).shape[0])) + \n",
    "      \" measurements have been removed from the dataset \\n\")\n",
    "\n",
    "\n",
    "print(data_cleaned_new.head())\n",
    "\n",
    "print(\"\\n Your new compiled dataset is available under the name of data_cleaned_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to check the number of numbers in defined interval\n",
    "\n",
    "#Select dataset (data_cleaned or data_cleaned_new)\n",
    "data_input = data_cleaned_new\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + str(list(data_cleaned.columns)) \n",
    "                    + \" : \")\n",
    "#Select values\n",
    "value_1 = int(input(\"Select lower interval value: \"))\n",
    "value_2 = int(input(\"Select higher interval value: \"))\n",
    "\n",
    "values_in_interval = data_input[column_name].between(value_1, value_2, inclusive = True)\n",
    "print(\"\\033[1m There are \" + str((data_input[values_in_interval]).shape[0]) + \" \" + str(column_name) +\n",
    "      \" values between \" + str(value_1) + \" and \" + str(value_2) + \" in the given data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Histogram Universal Code\n",
    "\n",
    "#Select dataset you want to work with (either data_cleaned or data_cleaned_new after removal of outliers)\n",
    "data_input = data_cleaned_new\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + str(list(data_cleaned.columns)) + \" : \")\n",
    "hist_bins = int(input(\"Enter number of bins in histogram \"))\n",
    "\n",
    "hist_variable = go.Figure(data=[go.Histogram(x=(data_input[column_name]), nbinsx=hist_bins)])\n",
    "\n",
    "hist_variable.update_layout(height = 450, width = 800,\n",
    "    xaxis_title=column_name,\n",
    "    yaxis_title=\"Frequency\"\n",
    ")\n",
    "\n",
    "hist_variable.show()\n",
    "\n",
    "\n",
    "#Lowest/Highest Value\n",
    "print(\"Lowest \" + column_name + \" in dataset is \" + str((data_input[column_name]).min()))\n",
    "\n",
    "print(\"Highest \" + column_name + \" in dataset is \" + str((data_input[column_name]).max()))\n",
    "\n",
    "\n",
    "\n",
    "#Histograms Between Severity Groups\n",
    "\n",
    "sev1 = ((data_input[(data_input['Severity']) == 1])[column_name])\n",
    "sev2 = ((data_input[(data_input['Severity']) == 2])[column_name])\n",
    "sev3 = ((data_input[(data_input['Severity']) == 3])[column_name])\n",
    "sev4 = ((data_input[(data_input['Severity']) == 4])[column_name])\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Severity 1\",\"Severity 2\", \"Severity 3\", \"Severity 4\"))\n",
    "\n",
    "#Adding Traces\n",
    "fig.add_trace(go.Histogram(x=(sev1), nbinsx=hist_bins), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=(sev2), nbinsx=hist_bins), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=(sev3), nbinsx=hist_bins), row=2, col=1)\n",
    "fig.add_trace(go.Histogram(x=(sev4), nbinsx=hist_bins), row=2, col=2)\n",
    "\n",
    "# Update xaxis properties\n",
    "fig.update_xaxes(title_text=column_name, row=1, col=1)\n",
    "fig.update_xaxes(title_text=column_name, row=1, col=2)\n",
    "fig.update_xaxes(title_text=column_name, row=2, col=1)\n",
    "fig.update_xaxes(title_text=column_name, row=2, col=2)\n",
    "\n",
    "# Update yaxis properties\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "\n",
    "# Update title and height\n",
    "fig.update_layout(title_text=\"Variable \" + column_name + \" histograms for each Severity group\", height=800)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box Plot Universal\n",
    "\n",
    "#Dataset slicing function - Universal - for outliers removal\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + str(list(data_cleaned.columns)) + \" : \")\n",
    "\n",
    "lower_value = int(input(\"Enter lowest included value in your chosen column for your new dataset \"))\n",
    "higher_value = int(input(\"Enter highest included value in your chosen column for your new dataset \"))\n",
    "data_cleaned_bool = data_cleaned[column_name].between(lower_value, higher_value, inclusive = True)\n",
    "data_cleaned_new = data_cleaned[data_cleaned_bool]\n",
    "print(str(((data_cleaned[data_cleaned[column_name] > higher_value]).shape[0])+\n",
    "          ((data_cleaned[data_cleaned[column_name] < lower_value]).shape[0])) + \n",
    "      \" measurements have been removed from the dataset \\n\")\n",
    "\n",
    "\n",
    "print(data_cleaned_new.head())\n",
    "\n",
    "print(\"\\n Your new compiled dataset is available under the name of data_cleaned_new\")\n",
    "\n",
    "#Choose data input (either data_cleaned or data_cleaned_new (after outlier removal))\n",
    "data_input = data_cleaned_new\n",
    "\n",
    "#box_plot_temperature = px.box(data_input, x=\"Severity\", y=column_name)\n",
    "#box_plot_temperature.show()\n",
    "\n",
    "\n",
    "#Median + Mean Calculation\n",
    "Median_results = {}\n",
    "Mean_results = {}\n",
    "sd_results = {}\n",
    "sem_results= {}\n",
    "\n",
    "for i, u in enumerate(range(1,5), 1):\n",
    "    Median_results['Severity' + str(u)] = (((data_input[data_input['Severity'] == i])[column_name]).median())\n",
    "    Mean_results['Severity' + str(u)] = (((data_input[data_input['Severity'] == i])[column_name]).mean())\n",
    "    sd_results['Severity' + str(u)] = (((data_input[data_input['Severity'] == i])[column_name]).std())\n",
    "    sem_results['Severity' + str(u)] = (((data_input[data_input['Severity'] == i])[column_name]).sem())\n",
    "\n",
    "    \n",
    "Mean_pd = pd.DataFrame.from_dict(Mean_results, orient='index')\n",
    "Mean_pd.columns = ['Mean']\n",
    "\n",
    "Median_pd = pd.DataFrame.from_dict(Median_results, orient='index')\n",
    "Median_pd.columns = ['Median']\n",
    "\n",
    "sd_pd = pd.DataFrame.from_dict(sd_results, orient='index')\n",
    "sd_pd.columns = ['Standard Deviation']\n",
    "\n",
    "sem_pd = pd.DataFrame.from_dict(sem_results, orient = 'index')\n",
    "sem_pd.columns = ['Standard Error']\n",
    "\n",
    "result = Mean_pd.join(Median_pd)\n",
    "\n",
    "result2 = result.join(sd_pd)\n",
    "\n",
    "result_final = result2.join(sem_pd)\n",
    "\n",
    "print(result_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anderson-Darling Test Universal\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import anderson\n",
    "\n",
    "#Choose dataset with what to work (data_cleaned or data_cleaned_new)\n",
    "data_input = data_cleaned_new\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + \n",
    "                    str(list(data_cleaned.columns)) + \" : \")\n",
    "\n",
    "sev1_and = ((data_input[(data_input['Severity']) == 1])[column_name])\n",
    "Severity1 = sev1_and.to_numpy()\n",
    "Severity1 = Severity1[np.logical_not(np.isnan(Severity1))]\n",
    "\n",
    "sev2_and = ((data_input[(data_input['Severity']) == 2])[column_name])\n",
    "Severity2 = sev2_and.to_numpy()\n",
    "Severity2 = Severity2[np.logical_not(np.isnan(Severity2))]\n",
    "\n",
    "sev3_and = ((data_input[(data_input['Severity']) == 3])[column_name])\n",
    "Severity3 = sev3_and.to_numpy()\n",
    "Severity3 = Severity3[np.logical_not(np.isnan(Severity3))]\n",
    "\n",
    "sev4_and = ((data_input[(data_input['Severity']) == 4])[column_name])\n",
    "Severity4 = sev4_and.to_numpy()\n",
    "Severity4 = Severity4[np.logical_not(np.isnan(Severity4))]\n",
    "\n",
    "sev_list = [Severity1, Severity2, Severity3, Severity4]\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in sev_list:\n",
    "    result = anderson(i)\n",
    "    print(\"\\033[1m\" + (namestr(i, globals())[0] + ' Statistic: %.3f \\033[0m' % result.statistic))\n",
    "    while counter <1:\n",
    "        p = 0\n",
    "        for i in range(len(result.critical_values)):\n",
    "            sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "            if result.statistic < result.critical_values[i]:\n",
    "                print('Significance level: %.3f: Critical Value : %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n",
    "            else:\n",
    "                print('Significance level: %.3f: Critical Value : %.3f, data does not look normal (reject H0)' % (sl, cv))\n",
    "            counter += 1\n",
    "    counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ANOVA universal\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "#Choose dataset with what to work (data_cleaned or data_cleaned_new)\n",
    "data_input = data_cleaned\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + \n",
    "                    str(list(data_cleaned.columns)) + \" : \")\n",
    "    \n",
    "    \n",
    "#Removing Outliers\n",
    "\n",
    "#Select values\n",
    "\n",
    "lower_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select lower interval value for slicing: \"))\n",
    "higher_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select higher interval value for slicing: \"))\n",
    "data_cleaned_bool = data_cleaned[column_name].between(lower_value, higher_value, inclusive = True)\n",
    "data_cleaned_new = data_cleaned[data_cleaned_bool]\n",
    "print(str(((data_cleaned[data_cleaned[column_name] > higher_value]).shape[0])+\n",
    "          ((data_cleaned[data_cleaned[column_name] < lower_value]).shape[0])) + \n",
    "      \" measurements have been removed from the dataset \\n\")\n",
    "\n",
    "print(\"\\n \\033[1m Your new compiled dataset is available under the name of data_cleaned_new \\033[0m\")\n",
    "\n",
    "\n",
    "#Extracting Severity groups from new dataset without outliers\n",
    "\n",
    "sev1_anova_no_outliers = ((data_cleaned_new[(data_cleaned_new['Severity']) == 1])[column_name])\n",
    "Severity1_no_outliers = sev1_anova_no_outliers.to_numpy()\n",
    "Severity1_no_outliers = Severity1_no_outliers[np.logical_not(np.isnan(Severity1_no_outliers))]\n",
    "\n",
    "sev2_anova_no_outliers = ((data_cleaned_new[(data_cleaned_new['Severity']) == 2])[column_name])\n",
    "Severity2_no_outliers = sev2_anova_no_outliers.to_numpy()\n",
    "Severity2_no_outliers = Severity2_no_outliers[np.logical_not(np.isnan(Severity2_no_outliers))]\n",
    "\n",
    "sev3_anova_no_outliers = ((data_cleaned_new[(data_cleaned_new['Severity']) == 3])[column_name])\n",
    "Severity3_no_outliers = sev3_anova_no_outliers.to_numpy()\n",
    "Severity3_no_outliers = Severity3_no_outliers[np.logical_not(np.isnan(Severity3_no_outliers))]\n",
    "\n",
    "sev4_anova_no_outliers = ((data_cleaned_new[(data_cleaned_new['Severity']) == 4])[column_name])\n",
    "Severity4_no_outliers = sev4_anova_no_outliers.to_numpy()\n",
    "Severity4_no_outliers = Severity4_no_outliers[np.logical_not(np.isnan(Severity4_no_outliers))]\n",
    "\n",
    "\n",
    "#ANOVA (outliers + no outliers)\n",
    "\n",
    "\n",
    "#ANOVA\n",
    "\n",
    "F, p = stats.f_oneway(Severity1_no_outliers, Severity2_no_outliers, Severity3_no_outliers, Severity4_no_outliers)\n",
    "print(str(column_name) + ' variable ANOVA value F is equal to ' + str(F))\n",
    "print(str(column_name) + ' variable ANOVA value p is equal to ' + str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Kruskal Wallis Universal\n",
    "\n",
    "\n",
    "#Choose dataset with what to work (data_cleaned or data_cleaned_new)\n",
    "data_input = data_cleaned\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + \n",
    "                    str(list(data_cleaned.columns)) + \" : \")\n",
    "    \n",
    "    \n",
    "#Removing Outliers\n",
    "\n",
    "#Select values\n",
    "\n",
    "lower_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select lower interval value for slicing: \"))\n",
    "higher_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select higher interval value for slicing: \"))\n",
    "data_cleaned_bool = data_cleaned[column_name].between(lower_value, higher_value, inclusive = True)\n",
    "data_cleaned_new = data_cleaned[data_cleaned_bool]\n",
    "print(str(((data_cleaned[data_cleaned[column_name] > higher_value]).shape[0])+\n",
    "          ((data_cleaned[data_cleaned[column_name] < lower_value]).shape[0])) + \n",
    "      \" measurements have been removed from the dataset \\n\")\n",
    "\n",
    "print(\"\\n \\033[1m Your new compiled dataset is available under the name of data_cleaned_new \\033[0m\")\n",
    "\n",
    "\n",
    "#Extracting Severity groups from new dataset without outliers\n",
    "\n",
    "sev1_anova_no_outliers = ((data_cleaned_new[(data_cleaned_new['Severity']) == 1])[column_name])\n",
    "Severity1_no_outliers = sev1_anova_no_outliers.to_numpy()\n",
    "Severity1_no_outliers = Severity1_no_outliers[np.logical_not(np.isnan(Severity1_no_outliers))]\n",
    "\n",
    "sev2_anova_no_outliers = ((data_cleaned_new[(data_cleaned_new['Severity']) == 2])[column_name])\n",
    "Severity2_no_outliers = sev2_anova_no_outliers.to_numpy()\n",
    "Severity2_no_outliers = Severity2_no_outliers[np.logical_not(np.isnan(Severity2_no_outliers))]\n",
    "\n",
    "sev3_anova_no_outliers = ((data_cleaned_new[(data_cleaned_new['Severity']) == 3])[column_name])\n",
    "Severity3_no_outliers = sev3_anova_no_outliers.to_numpy()\n",
    "Severity3_no_outliers = Severity3_no_outliers[np.logical_not(np.isnan(Severity3_no_outliers))]\n",
    "\n",
    "sev4_anova_no_outliers = ((data_cleaned_new[(data_cleaned_new['Severity']) == 4])[column_name])\n",
    "Severity4_no_outliers = sev4_anova_no_outliers.to_numpy()\n",
    "Severity4_no_outliers = Severity4_no_outliers[np.logical_not(np.isnan(Severity4_no_outliers))]\n",
    "\n",
    "#Kruskal-Wallis\n",
    "\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "stat, p = kruskal(Severity1_no_outliers, Severity2_no_outliers, Severity3_no_outliers, Severity4_no_outliers)\n",
    "print(str(column_name) + ' variable Kruskal-Wallis Statistics value is equal to ' + str(stat))\n",
    "print(str(column_name) + ' variable Kruskal-Wallis p value is equal to ' + str(p))\n",
    "\n",
    "\n",
    "# Mann-Whitney U test\n",
    "from scipy.stats import mannwhitneyu\n",
    "stat, p = mannwhitneyu(Severity2_no_outliers, Severity3_no_outliers)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Same distribution (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different distribution (reject H0)')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Z-test Universal\n",
    "\n",
    "from statsmodels.stats import weightstats as stests\n",
    "\n",
    "#Choose dataset with what to work (data_cleaned or data_cleaned_new)\n",
    "data_input = data_cleaned\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + \n",
    "                    str(list(data_cleaned.columns)) + \" : \")\n",
    "    \n",
    "    \n",
    "#Removing Outliers\n",
    "\n",
    "#Select values\n",
    "\n",
    "lower_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select lower interval value for slicing: \"))\n",
    "higher_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select higher interval value for slicing: \"))\n",
    "data_cleaned_bool = data_cleaned[column_name].between(lower_value, higher_value, inclusive = True)\n",
    "data_cleaned_new = data_cleaned[data_cleaned_bool]\n",
    "print(str(((data_cleaned[data_cleaned[column_name] > higher_value]).shape[0])+\n",
    "          ((data_cleaned[data_cleaned[column_name] < lower_value]).shape[0])) + \n",
    "      \" measurements have been removed from the dataset \\n\")\n",
    "\n",
    "print(\"\\n \\033[1m Your new compiled dataset is available under the name of data_cleaned_new \\033[0m\")\n",
    "\n",
    "data_input = data_cleaned_new\n",
    "\n",
    "sev1_and = ((data_input[(data_input['Severity']) == 1])[column_name])\n",
    "Severity1 = sev1_and.to_numpy()\n",
    "Severity1 = Severity1[np.logical_not(np.isnan(Severity1))]\n",
    "\n",
    "sev2_and = ((data_input[(data_input['Severity']) == 2])[column_name])\n",
    "Severity2 = sev2_and.to_numpy()\n",
    "Severity2 = Severity2[np.logical_not(np.isnan(Severity2))]\n",
    "\n",
    "sev3_and = ((data_input[(data_input['Severity']) == 3])[column_name])\n",
    "Severity3 = sev3_and.to_numpy()\n",
    "Severity3 = Severity3[np.logical_not(np.isnan(Severity3))]\n",
    "\n",
    "sev4_and = ((data_input[(data_input['Severity']) == 4])[column_name])\n",
    "Severity4 = sev4_and.to_numpy()\n",
    "Severity4 = Severity4[np.logical_not(np.isnan(Severity4))]\n",
    "\n",
    "\n",
    "sev_all = [Severity1, Severity2, Severity3, Severity4]\n",
    "sev_all2 = [Severity2, Severity3, Severity4]\n",
    "\n",
    "z_statistics={}\n",
    "z_p_values={}\n",
    "\n",
    "    \n",
    "counter = 0\n",
    "\n",
    "for i in sev_all:\n",
    "    while sev_all2 and counter <1:\n",
    "        for u in sev_all2:\n",
    "            ztest, pval = stests.ztest(i, u, value=0, alternative='two-sided')\n",
    "            if pval == 1 or ztest == 0:\n",
    "                pass\n",
    "            else:\n",
    "                z_p_values[namestr(i, globals())[1]+ \" vs \" + namestr(u, globals())[1]]=pval\n",
    "                z_statistics[namestr(i, globals())[1]+ \" vs \" + namestr(u, globals())[1]]=ztest\n",
    "                counter += 1\n",
    "    counter = 0\n",
    "    \n",
    "\n",
    "z_test_pd1 = pd.DataFrame.from_dict(z_statistics, orient='index')\n",
    "z_test_pd1.columns = ['Z Statistics']\n",
    "\n",
    "z_test_pd2 = pd.DataFrame.from_dict(z_p_values, orient='index')\n",
    "z_test_pd2.columns = ['Z P Value']\n",
    "\n",
    "result_z = z_test_pd1.join(z_test_pd2)\n",
    "\n",
    "result_z = result_z.drop([result_z.index[5], result_z.index[7], result_z.index[8]])\n",
    "print(result_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mann Whitney U non parametric\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "\n",
    "#Choose dataset with what to work (data_cleaned or data_cleaned_new)\n",
    "data_input = data_cleaned\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + \n",
    "                    str(list(data_cleaned.columns)) + \" : \")\n",
    "    \n",
    "    \n",
    "#Removing Outliers\n",
    "\n",
    "#Select values\n",
    "\n",
    "lower_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select lower interval value for slicing: \"))\n",
    "higher_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select higher interval value for slicing: \"))\n",
    "data_cleaned_bool = data_cleaned[column_name].between(lower_value, higher_value, inclusive = True)\n",
    "data_cleaned_new = data_cleaned[data_cleaned_bool]\n",
    "print(str(((data_cleaned[data_cleaned[column_name] > higher_value]).shape[0])+\n",
    "          ((data_cleaned[data_cleaned[column_name] < lower_value]).shape[0])) + \n",
    "      \" measurements have been removed from the dataset \\n\")\n",
    "\n",
    "print(\"\\n \\033[1m Your new compiled dataset is available under the name of data_cleaned_new \\033[0m\")\n",
    "\n",
    "data_input = data_cleaned_new\n",
    "\n",
    "sev1_and = ((data_input[(data_input['Severity']) == 1])[column_name])\n",
    "Severity1 = sev1_and.to_numpy()\n",
    "Severity1 = Severity1[np.logical_not(np.isnan(Severity1))]\n",
    "\n",
    "sev2_and = ((data_input[(data_input['Severity']) == 2])[column_name])\n",
    "Severity2 = sev2_and.to_numpy()\n",
    "Severity2 = Severity2[np.logical_not(np.isnan(Severity2))]\n",
    "\n",
    "sev3_and = ((data_input[(data_input['Severity']) == 3])[column_name])\n",
    "Severity3 = sev3_and.to_numpy()\n",
    "Severity3 = Severity3[np.logical_not(np.isnan(Severity3))]\n",
    "\n",
    "sev4_and = ((data_input[(data_input['Severity']) == 4])[column_name])\n",
    "Severity4 = sev4_and.to_numpy()\n",
    "Severity4 = Severity4[np.logical_not(np.isnan(Severity4))]\n",
    "\n",
    "\n",
    "sev_all = [Severity1, Severity2, Severity3, Severity4]\n",
    "sev_all2 = [Severity2, Severity3, Severity4]\n",
    "\n",
    "\n",
    "\n",
    "#U TEST\n",
    "\n",
    "U_stat_values={}\n",
    "U_p_values={}\n",
    "    \n",
    "counter = 0\n",
    "\n",
    "for i in sev_all:\n",
    "    while counter <1:\n",
    "        for u in sev_all2:\n",
    "            stat, p = mannwhitneyu(i, u)\n",
    "            if pval == 1 or ztest == 0:\n",
    "                pass\n",
    "            else:\n",
    "                U_p_values[namestr(i, globals())[1]+ \" vs \" + namestr(u, globals())[1]]=p\n",
    "                U_stat_values[namestr(i, globals())[1]+ \" vs \" + namestr(u, globals())[1]]=stat\n",
    "                counter += 1\n",
    "    counter = 0\n",
    "    \n",
    "\n",
    "U_stat_pd1 = pd.DataFrame.from_dict(U_stat_values, orient='index')\n",
    "U_stat_pd1.columns = ['Statistics']\n",
    "\n",
    "U_p_pd2 = pd.DataFrame.from_dict(U_p_values, orient='index')\n",
    "U_p_pd2.columns = ['P Value']\n",
    "\n",
    "result_U = U_stat_pd1.join(U_p_pd2)\n",
    "\n",
    "result_U = result_U.drop([result_U.index[3], result_U.index[6], result_U.index[8], result_U.index[9]])\n",
    "print(result_U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select values\n",
    "\n",
    "lower_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select lower interval value for slicing: \"))\n",
    "higher_value = int(input(\"Slicing dataset... Removing Outliers... Creating new dataset data_set_new... Select higher interval value for slicing: \"))\n",
    "data_cleaned_bool = data_cleaned[column_name].between(lower_value, higher_value, inclusive = True)\n",
    "data_cleaned_new = data_cleaned[data_cleaned_bool]\n",
    "print(str(((data_cleaned[data_cleaned[column_name] > higher_value]).shape[0])+\n",
    "          ((data_cleaned[data_cleaned[column_name] < lower_value]).shape[0])) + \n",
    "      \" measurements have been removed from the dataset \\n\")\n",
    "\n",
    "print(\"\\n \\033[1m Your new compiled dataset is available under the name of data_cleaned_new \\033[0m\")\n",
    "\n",
    "data_input = data_cleaned_new\n",
    "\n",
    "stat, p = mannwhitneyu(Severity1, Severity4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function\n",
    "#\n",
    "#list = [\"a\", \"b\", \"c\", \"d\"]\n",
    "#list2 = [\"b\", \"c\", \"d\"]\n",
    "#\n",
    "#counter = 0\n",
    "#\n",
    "#for i in list:\n",
    "#    while counter <1:\n",
    "#        for u in list2:\n",
    "#            if i == u:\n",
    "#                pass\n",
    "#            else:\n",
    "#                print(i+u) \n",
    "#            counter += 1\n",
    "#    counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned_final = data_cleaned[data_cleaned['Temperature(C)'].between(-55, 50, inclusive = True)]\n",
    "\n",
    "data_cleaned_final = data_cleaned_final[data_cleaned_final['Pressure(mbar)'].between(950, 1055, inclusive = True)]\n",
    "\n",
    "data_cleaned_final = data_cleaned_final[data_cleaned_final['Wind_Speed(kmh)'].between(0, 150, inclusive = True)]\n",
    "\n",
    "data_cleaned_final = data_cleaned_final[data_cleaned_final['Visibility(km)'].between(0, 80, inclusive = True)]\n",
    "\n",
    "\n",
    "print(data_cleaned_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + \n",
    "                    str(list(data_cleaned.columns)) + \" : \")\n",
    "\n",
    "data_sev_temp = data_cleaned_final[['Severity', column_name]]\n",
    "                          \n",
    "data_sev_temp = data_sev_temp.dropna()\n",
    "\n",
    "data_sev_temp = data_sev_temp.sort_values(by=['Severity'])\n",
    "\n",
    "data_sev_temp2 = data_sev_temp.drop(columns='Severity')\n",
    "\n",
    "data_sev_temp = (data_sev_temp['Severity']).to_numpy()\n",
    "\n",
    "#Ordinal Logistic Regression\n",
    "\n",
    "from bevel.linear_ordinal_regression import OrderedLogit\n",
    "\n",
    "ls = OrderedLogit()\n",
    "ls.fit(data_sev_temp2, data_sev_temp)\n",
    "\n",
    "ls.print_summary()\n",
    "\n",
    "result = ls.predict_probabilities(data_sev_temp2)\n",
    "\n",
    "print(result[:][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNIVERSAL CODE\n",
    "\n",
    "column_name = input(\"Enter column name without brackets. Possible choices: \" + \n",
    "                    str(list(data_cleaned.columns)) + \" : \")\n",
    "\n",
    "#Extract array of column_name variable values\n",
    "\n",
    "variable_array = np.array(data_sev_temp2[column_name])\n",
    "print(variable_array)\n",
    "\n",
    "#Extract Probability Array Values\n",
    "\n",
    "list_res = []\n",
    "\n",
    "for i in range(0, len(result)):\n",
    "   list_res.append(result[i][3])\n",
    "\n",
    "Probability_array = np.array(list_res)\n",
    "print(Probability_array)\n",
    "\n",
    "\n",
    "#Linear Regression\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "variable_array_regression = variable_array.reshape((-1, 1))\n",
    "\n",
    "model = LinearRegression().fit(variable_array_regression, Probability_array)\n",
    "\n",
    "print(model)\n",
    "\n",
    "r_sq = model.score(variable_array_regression, Probability_array)\n",
    "intercept = model.intercept_\n",
    "slope = model.coef_\n",
    "\n",
    "\n",
    "#Line for plotting\n",
    "\n",
    "line = slope*variable_array+intercept\n",
    "print(line)\n",
    "\n",
    "#Create DataFrame from column_name variable and Probability\n",
    "\n",
    "pd_result = pd.DataFrame({column_name:variable_array, 'Probability':Probability_array})\n",
    "print(pd_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(pd_result['Temperature(C)'], pd_result['Probability'])\n",
    "plt.plot(Temperature_array, line, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(pd_result, x=column_name, y=\"Probability\")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=variable_array,\n",
    "        y=line,\n",
    "        mode=\"lines\",\n",
    "        line=go.scatter.Line(color=\"gray\"),\n",
    "        showlegend=False)\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print('Coefficient of determination (R^2 value): ' + str(r_sq))\n",
    "print('Slope : ' + str(slope))\n",
    "print('Intercept : ' + str(intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram Temperature\n",
    "hist_temperature = go.Figure(data=[go.Histogram(x=(data_cleaned[t_col]), nbinsx=30)])\n",
    "\n",
    "hist_temperature.update_layout(height = 450, width = 800,\n",
    "    xaxis_title=\"Temperature (\\u00b0C)\",\n",
    "    yaxis_title=\"Frequency\"\n",
    ")\n",
    "\n",
    "hist_temperature.show()\n",
    "\n",
    "\n",
    "\n",
    "#Lowest/Highest Temp\n",
    "print(\"Lowest temperature in dataset is \" + str((data_cleaned[t_col]).min()) + \" \\u00b0C\")\n",
    "\n",
    "print(\"Highest temperature in dataset is \" + str((data_cleaned[t_col]).max()) + \" \\u00b0C\")\n",
    "\n",
    "\n",
    "\n",
    "#Left and right member in the low temperature interval\n",
    "t_low1 = -70\n",
    "t_low2 = -55\n",
    "\n",
    "temperature_values_low = data_cleaned[t_col].between(t_low1, t_low2, inclusive = True)\n",
    "print(\"There are \" + str((data_cleaned[temperature_values_low]).shape[0]) + \n",
    "      \" temperature values between \" + str(t_low1) + \" and \" + str(t_low2) + \" \\u00b0C\")\n",
    "\n",
    "#Left member in the interval\n",
    "t_high1 = 55\n",
    "\n",
    "#Right number in the interval\n",
    "t_high2 = 80\n",
    "\n",
    "temperature_values_high = data_cleaned[t_col].between(t_high1, t_high2, inclusive = True)\n",
    "\n",
    "print(\"There are \" + str((data_cleaned[temperature_values_high]).shape[0]) + \n",
    "      \" temperature values between \" + str(t_high1) + \" and \" + str(t_high2) + \" \\u00b0C\")\n",
    "\n",
    "\n",
    "#Outliers (extremes) removal\n",
    "data_cleaned_bool = data_cleaned[t_col].between(-55, 55, inclusive = True)\n",
    "data_cleaned_temp = data_cleaned[data_cleaned_bool]\n",
    "print(str(((data_cleaned[data_cleaned[t_col] > 55]).shape[0])+((data_cleaned[data_cleaned[t_col] < -55]).shape[0])) + \n",
    "      \" measurements have been removed from the dataset \")\n",
    "data_cleaned.head()\n",
    "\n",
    "#Temperature Between Severity Groups Histograms\n",
    "\n",
    "#Severity 1\n",
    "\n",
    "sev1_temp = ((data_cleaned_temp[(data_cleaned_temp['Severity']) == 1])[t_col])\n",
    "sev2_temp = ((data_cleaned_temp[(data_cleaned_temp['Severity']) == 2])[t_col])\n",
    "sev3_temp = ((data_cleaned_temp[(data_cleaned_temp['Severity']) == 3])[t_col])\n",
    "sev4_temp = ((data_cleaned_temp[(data_cleaned_temp['Severity']) == 4])[t_col])\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Severity 1\",\"Severity 2\", \"Severity 3\", \"Severity 4\"))\n",
    "\n",
    "#Adding Traces\n",
    "fig.add_trace(go.Histogram(x=(sev1_temp), nbinsx=20), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=(sev2_temp), nbinsx=20), row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=(sev3_temp), nbinsx=20), row=2, col=1)\n",
    "fig.add_trace(go.Histogram(x=(sev4_temp), nbinsx=20), row=2, col=2)\n",
    "\n",
    "# Update xaxis properties\n",
    "fig.update_xaxes(title_text=\"Temperature \\u00b0C\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Temperature \\u00b0C\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Temperature \\u00b0C\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Temperature \\u00b0C\", row=2, col=2)\n",
    "\n",
    "# Update yaxis properties\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=2)\n",
    "\n",
    "# Update title and height\n",
    "fig.update_layout(title_text=\"Variable Temperature \\u00b0C histograms for each severity group\", height=800)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anderson-Darling Test Temperature\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "sev1_temp = ((data_cleaned_temp[(data_cleaned_temp['Severity']) == 1])[t_col])\n",
    "sev1_temp_np = sev1_temp.to_numpy()\n",
    "sev1_temp_np = sev1_temp_np[np.logical_not(np.isnan(sev1_temp_np))]\n",
    "\n",
    "from scipy.stats import anderson\n",
    "\n",
    "#1\n",
    "result = anderson(sev1_temp_np)\n",
    "stat = round(result.statistic, 4)\n",
    "\n",
    "p = 0\n",
    "result_mat = []\n",
    "for i in range(len(result.critical_values)):\n",
    "    title = \"Severity \" + str(int((i+3)%(i+2)))\n",
    "    sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < result.critical_values[i]:\n",
    "        msg = 'Sample looks Gaussian (fail to reject H0)'\n",
    "    else:\n",
    "        msg = 'Sample does not look Gaussian (reject H0)'\n",
    "    result_mat.append([title, len(sev1_temp_np), stat, sl, cv, msg])\n",
    "\n",
    "trace = go.Table(\n",
    "    header=dict(values=['<b>Title<b>', '<b>Sample Size</b>', '<b>Statistic</b>', '<b>Significance Level</b>', '<b>Critical Value</b>', '<b>Comment</b>'],\n",
    "                line = dict(width=0),\n",
    "                fill = dict(color='rgba(42,63,95,0.8)'),\n",
    "                align = 'center',\n",
    "                font = dict(\n",
    "                    color = '#ffffff',\n",
    "                    size = 12\n",
    "                )),\n",
    "    cells=dict(values=np.array(result_mat).T,\n",
    "               line = dict(width=0),\n",
    "               fill = dict(color=[['#EBF0F8', '#ffffff', '#EBF0F8', '#ffffff', '#EBF0F8']]),\n",
    "               align = 'center',\n",
    "               height = 40),\n",
    "    columnwidth=[0.3, 0.25, 0.3, 0.25, 0.5])\n",
    "layout = dict(\n",
    "    height=300,\n",
    "    margin=dict(\n",
    "        l=5,\n",
    "        r=5,\n",
    "        t=30,\n",
    "        b=0\n",
    "    )\n",
    ")\n",
    "data = [trace]\n",
    "andar_table = dict(data=data, layout=layout)\n",
    "\n",
    "iplot(andar_table, filename='anderson-darling-table')\n",
    "\n",
    "#2\n",
    "\n",
    "sev2_temp = ((data_cleaned_temp[(data_cleaned_temp['Severity']) == 2])[t_col])\n",
    "sev2_temp_np = sev2_temp.to_numpy()\n",
    "sev2_temp_np = sev2_temp_np[np.logical_not(np.isnan(sev2_temp_np))]\n",
    "\n",
    "result = anderson(sev2_temp_np)\n",
    "stat = round(result.statistic, 4)\n",
    "\n",
    "p = 0\n",
    "result_mat = []\n",
    "for i in range(len(result.critical_values)):\n",
    "    title = \"Severity \" + str(int((i+6)%(i+4)))\n",
    "    sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < result.critical_values[i]:\n",
    "        msg = 'Sample looks Gaussian (fail to reject H0)'\n",
    "    else:\n",
    "        msg = 'Sample does not look Gaussian (reject H0)'\n",
    "    result_mat.append([title, len(sev2_temp_np), stat, sl, cv, msg])\n",
    "\n",
    "trace = go.Table(\n",
    "    header=dict(values=['<b>Title<b>', '<b>Sample Size</b>', '<b>Statistic</b>', '<b>Significance Level</b>', '<b>Critical Value</b>', '<b>Comment</b>'],\n",
    "                line = dict(width=0),\n",
    "                fill = dict(color='rgba(42,63,95,0.8)'),\n",
    "                align = 'center',\n",
    "                font = dict(\n",
    "                    color = '#ffffff',\n",
    "                    size = 12\n",
    "                )),\n",
    "    cells=dict(values=np.array(result_mat).T,\n",
    "               line = dict(width=0),\n",
    "               fill = dict(color=[['#EBF0F8', '#ffffff', '#EBF0F8', '#ffffff', '#EBF0F8']]),\n",
    "               align = 'center',\n",
    "               height = 40),\n",
    "    columnwidth=[0.3, 0.25, 0.3, 0.25, 0.5])\n",
    "layout = dict(\n",
    "    height=300,\n",
    "    margin=dict(\n",
    "        l=5,\n",
    "        r=5,\n",
    "        t=30,\n",
    "        b=0\n",
    "    )\n",
    ")\n",
    "data = [trace]\n",
    "andar_table = dict(data=data, layout=layout)\n",
    "\n",
    "iplot(andar_table, filename='anderson-darling-table')\n",
    "\n",
    "#3\n",
    "\n",
    "sev3_temp = ((data_cleaned_temp[(data_cleaned_temp['Severity']) == 3])[t_col])\n",
    "sev3_temp_np = sev3_temp.to_numpy()\n",
    "sev3_temp_np = sev3_temp_np[np.logical_not(np.isnan(sev3_temp_np))]\n",
    "\n",
    "result = anderson(sev3_temp_np)\n",
    "stat = round(result.statistic, 4)\n",
    "\n",
    "p = 0\n",
    "result_mat = []\n",
    "for i in range(len(result.critical_values)):\n",
    "    title = \"Severity \" + str(int((i+7)%(i+4)))\n",
    "    sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < result.critical_values[i]:\n",
    "        msg = 'Sample looks Gaussian (fail to reject H0)'\n",
    "    else:\n",
    "        msg = 'Sample does not look Gaussian (reject H0)'\n",
    "    result_mat.append([title, len(sev3_temp_np), stat, sl, cv, msg])\n",
    "\n",
    "trace = go.Table(\n",
    "    header=dict(values=['<b>Title<b>', '<b>Sample Size</b>', '<b>Statistic</b>', '<b>Significance Level</b>', '<b>Critical Value</b>', '<b>Comment</b>'],\n",
    "                line = dict(width=0),\n",
    "                fill = dict(color='rgba(42,63,95,0.8)'),\n",
    "                align = 'center',\n",
    "                font = dict(\n",
    "                    color = '#ffffff',\n",
    "                    size = 12\n",
    "                )),\n",
    "    cells=dict(values=np.array(result_mat).T,\n",
    "               line = dict(width=0),\n",
    "               fill = dict(color=[['#EBF0F8', '#ffffff', '#EBF0F8', '#ffffff', '#EBF0F8']]),\n",
    "               align = 'center',\n",
    "               height = 40),\n",
    "    columnwidth=[0.3, 0.25, 0.3, 0.25, 0.5])\n",
    "layout = dict(\n",
    "    height=300,\n",
    "    margin=dict(\n",
    "        l=5,\n",
    "        r=5,\n",
    "        t=30,\n",
    "        b=0\n",
    "    )\n",
    ")\n",
    "data = [trace]\n",
    "andar_table = dict(data=data, layout=layout)\n",
    "\n",
    "iplot(andar_table, filename='anderson-darling-table')\n",
    "\n",
    "#4\n",
    "\n",
    "sev4_temp = ((data_cleaned_temp[(data_cleaned_temp['Severity']) == 4])[t_col])\n",
    "sev4_temp_np = sev4_temp.to_numpy()\n",
    "sev4_temp_np = sev4_temp_np[np.logical_not(np.isnan(sev4_temp_np))]\n",
    "\n",
    "result = anderson(sev4_temp_np)\n",
    "stat = round(result.statistic, 4)\n",
    "\n",
    "p = 0\n",
    "result_mat = []\n",
    "for i in range(len(result.critical_values)):\n",
    "    title = \"Severity \" + str(int((i+9)%(i+5)))\n",
    "    sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < result.critical_values[i]:\n",
    "        msg = 'Sample looks Gaussian (fail to reject H0)'\n",
    "    else:\n",
    "        msg = 'Sample does not look Gaussian (reject H0)'\n",
    "    result_mat.append([title, len(sev4_temp_np), stat, sl, cv, msg])\n",
    "\n",
    "trace = go.Table(\n",
    "    header=dict(values=['<b>Title<b>', '<b>Sample Size</b>', '<b>Statistic</b>', '<b>Significance Level</b>', '<b>Critical Value</b>', '<b>Comment</b>'],\n",
    "                line = dict(width=0),\n",
    "                fill = dict(color='rgba(42,63,95,0.8)'),\n",
    "                align = 'center',\n",
    "                font = dict(\n",
    "                    color = '#ffffff',\n",
    "                    size = 12\n",
    "                )),\n",
    "    cells=dict(values=np.array(result_mat).T,\n",
    "               line = dict(width=0),\n",
    "               fill = dict(color=[['#EBF0F8', '#ffffff', '#EBF0F8', '#ffffff', '#EBF0F8']]),\n",
    "               align = 'center',\n",
    "               height = 40),\n",
    "    columnwidth=[0.3, 0.25, 0.3, 0.25, 0.5])\n",
    "layout = dict(\n",
    "    height=300,\n",
    "    margin=dict(\n",
    "        l=5,\n",
    "        r=5,\n",
    "        t=30,\n",
    "        b=0\n",
    "    )\n",
    ")\n",
    "data = [trace]\n",
    "andar_table = dict(data=data, layout=layout)\n",
    "\n",
    "iplot(andar_table, filename='anderson-darling-table')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
